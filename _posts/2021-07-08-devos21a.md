---
title: Model-Agnostic Learning to Meta-Learn
abstract: In this paper, we propose a learning algorithm that enables a model to quickly
  exploit commonalities among related tasks from an unseen task distribution, before
  quickly adapting to specific tasks from that same distribution. We investigate how
  learning with different task distributions can first improve adaptability by meta-finetuning
  on related tasks before improving goal task generalization with finetuning. Synthetic
  regression experiments validate the intuition that learning to meta-learn improves
  adaptability and consecutively generalization. Experiments on more complex image
  classification, continual regression, and reinforcement learning tasks demonstrate
  that learning to meta-learn generally improves task-specific adaptation. The methodology,
  setup, and hypotheses in this proposal were positively evaluated by peer review
  before conclusive experiments were carried out.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: devos21a
month: 0
tex_title: Model-Agnostic Learning to Meta-Learn
firstpage: 155
lastpage: 175
page: 155-175
order: 155
cycles: false
bibtex_author: Devos, Arnout and Dandi, Yatin
author:
- given: Arnout
  family: Devos
- given: Yatin
  family: Dandi
date: 2021-07-08
address:
container-title: NeurIPS 2020 Workshop on Pre-registration in Machine Learning
volume: '148'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 8
pdf: http://proceedings.mlr.press/v148/devos21a/devos21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
