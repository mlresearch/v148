---
title: Decomposing camera and object motion for an improved video sequence prediction
abstract: We propose a novel deep learning framework that focuses on decomposing the
  motion or the flow of the pixels from the background for an improved and longer
  prediction of video sequences. We propose to generate multi-timestep pixel level
  prediction using a framework that is trained to learn the temporal and spatial dependencies
  encoded in the video data separately. The proposed framework, called Velocity Acceleration
  Network or VANet, is capable of predicting long term video frames for a static scenario,
  where the camera is stationary, as well as in dynamic partially observable cases,
  where the camera is mounted on a moving platform (cars or robots). This framework
  decomposes the flow of the image sequences into velocity and acceleration maps and
  learns the temporal transformations using a convolutional LSTM network. Our detailed
  empirical study on three different datasets (BAIR, KTH and KITTI) shows that conditioning
  recurrent networks like LSTMs with higher order optical flow maps results in improved
  inference capabilities for videos.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sarkar21a
month: 0
tex_title: Decomposing camera and object motion for an improved video sequence prediction
firstpage: 358
lastpage: 374
page: 358-374
order: 358
cycles: false
bibtex_author: Sarkar, Meenakshi and Ghose, Debasish and Bala, Aniruddha
author:
- given: Meenakshi
  family: Sarkar
- given: Debasish
  family: Ghose
- given: Aniruddha
  family: Bala
date: 2021-07-08
address:
container-title: NeurIPS 2020 Workshop on Pre-registration in Machine Learning
volume: '148'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 8
pdf: http://proceedings.mlr.press/v148/sarkar21a/sarkar21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
